{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Colorization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBYk_XbIeqLH",
        "colab_type": "text"
      },
      "source": [
        "# Train the model to colorize images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z32qAcBNe2SL",
        "colab_type": "text"
      },
      "source": [
        "#Put images in a certain directory. \n",
        "## Prepare 6 train images and test images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6eAew6oVwahi",
        "colab": {}
      },
      "source": [
        "path = '/content/img'\n",
        "train_datagen = ImageDataGenerator(rescale=1. / 255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fb30c9c6-1cf8-4598-b1fd-4f7e29af89e2",
        "id": "6YiocVlqwahz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import glob\n",
        "import PIL\n",
        "from PIL import Image\n",
        "train_img = []\n",
        "train_dir = r'/content/img'\n",
        "for im in glob.glob(train_dir+'/*.jpg'):\n",
        "#for im in tr_data:\n",
        "    im1 = Image.open(im)\n",
        "    im = im1.resize((256,256),PIL.Image.ANTIALIAS)\n",
        "    train_img.append(im)\n",
        "print(train_img)\n",
        "print(len(train_img))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<PIL.Image.Image image mode=RGB size=256x256 at 0x7F036276FE80>, <PIL.Image.Image image mode=RGB size=256x256 at 0x7F03627C8D68>, <PIL.Image.Image image mode=RGB size=256x256 at 0x7F036269DEF0>, <PIL.Image.Image image mode=RGB size=256x256 at 0x7F036269DCF8>, <PIL.Image.Image image mode=RGB size=256x256 at 0x7F036269DC88>, <PIL.Image.Image image mode=RGB size=256x256 at 0x7F036269DCC0>]\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8njPHKr7yrq",
        "colab_type": "code",
        "outputId": "15945c32-23bc-4857-cbd4-0ea28417d9e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "test_img=[]\n",
        "test_dir = r'/content/test_img'\n",
        "for im in glob.glob(test_dir+'/*.jpg'):\n",
        "#for im in tr_data:\n",
        "    im1 = Image.open(im)\n",
        "    im = im1.resize((256,256),PIL.Image.ANTIALIAS)\n",
        "    test_img.append(im)\n",
        "X_test=[]\n",
        "for img in train_img:\n",
        "    lab = rgb2lab(img)\n",
        "    X_test.append(lab[:,:,0])\n",
        "#X_test = X_test.reshape(X_test.shape+(1,))\n",
        "print(len(X_test))\n",
        "print(X_test[1])\n",
        "print(X_test[1].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "[[35.24402364 36.2487985  38.9548529  ... 37.51177801 37.07615668\n",
            "  36.40400918]\n",
            " [35.65282586 37.47198985 40.12333705 ... 37.73252611 37.2929758\n",
            "  36.7605465 ]\n",
            " [36.06109832 38.3327681  40.66366364 ... 37.91139517 37.69211843\n",
            "  37.33396655]\n",
            " ...\n",
            " [19.96856806 19.6378602  19.34592902 ... 40.81491762 40.07095071\n",
            "  39.88901756]\n",
            " [20.23468874 19.56856679 19.73988082 ... 40.55245438 40.47583721\n",
            "  40.44929125]\n",
            " [20.18138154 19.96856806 19.99836765 ... 39.96476613 39.81140489\n",
            "  39.75085018]]\n",
            "(256, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b9189844-0826-4a67-e6dc-bde6b7adb084",
        "id": "KpsMs0icwah8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "X =[]\n",
        "Y =[]\n",
        "for img in train_img:\n",
        "  try:\n",
        "      lab = rgb2lab(img)\n",
        "      X.append(lab[:,:,0])\n",
        "      Y.append(lab[:,:,1:] / 128)\n",
        "  except:\n",
        "     print('error')\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "X = X.reshape(X.shape+(1,))\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 256, 256, 1)\n",
            "(6, 256, 256, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu6y1mvngJuA",
        "colab_type": "text"
      },
      "source": [
        "#Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hb5mpCMDhIk3",
        "colab": {}
      },
      "source": [
        "session = tf.Session()\n",
        "x = tf.placeholder(tf.float32, shape = [None, 256, 256, 1], name = 'x')\n",
        "#x=Input(shape=(256,256,1))\n",
        "ytrue = tf.placeholder(tf.float32, shape = [None, 256, 256, 2], name = 'ytrue')\n",
        "def create_weights(shape):\n",
        "  return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
        "def create_bias(size):\n",
        "  return tf.Variable(tf.constant(0.1, shape = [size]))\n",
        "def convolution(inputs, num_channels, filter_size, num_filters):\n",
        "  weights = create_weights(shape = [filter_size, filter_size, num_channels, num_filters])\n",
        "  bias = create_bias(num_filters)\n",
        "  \n",
        "  ## convolutional layer\n",
        "  layer = tf.nn.conv2d(input = inputs, filter = weights, strides= [1, 1, 1, 1], padding = 'SAME') + bias\n",
        "  layer = tf.nn.tanh(layer)\n",
        "  return layer\n",
        "def maxpool(inputs, kernel, stride):\n",
        "  layer = tf.nn.max_pool(value = inputs, ksize = [1, kernel, kernel, 1], strides = [1, stride, stride, 1], padding = \"SAME\")\n",
        "  return layer\n",
        "def upsampling(inputs):\n",
        "  layer = tf.image.resize_nearest_neighbor(inputs, (2*inputs.get_shape().as_list()[1], 2*inputs.get_shape().as_list()[2]))\n",
        "  return layer\n",
        "conv1 = convolution(x, 1, 3, 3)\n",
        "max1 = maxpool(conv1, 2, 2)\n",
        "conv2 = convolution(max1, 3, 3, 8)\n",
        "max2 = maxpool(conv2, 2, 2)\n",
        "conv3 = convolution(max2, 8, 3, 16)\n",
        "max3 = maxpool(conv3, 2, 2)\n",
        "conv4 = convolution(max3, 16, 3, 16)\n",
        "max4 = maxpool(conv4, 2, 2)\n",
        "conv5 = convolution(max4, 16, 3, 32)\n",
        "max5 = maxpool(conv5, 2, 2)\n",
        "conv6 = convolution(max5, 32, 3, 32)\n",
        "max6 = maxpool(conv6, 2, 2)\n",
        "conv7 = convolution(max6, 32, 3, 64)\n",
        "upsample1 = upsampling(conv7)\n",
        "conv8 = convolution(upsample1, 64, 3, 32)\n",
        "upsample2 = upsampling(conv8)\n",
        "conv9 = convolution(upsample2, 32, 3, 32)\n",
        "upsample3 = upsampling(conv9)\n",
        "conv10 = convolution(upsample3, 32, 3, 16)\n",
        "upsample4 = upsampling(conv10)\n",
        "conv11 = convolution(upsample4, 16, 3, 16)\n",
        "upsample5 = upsampling(conv11)\n",
        "conv12 = convolution(upsample5, 16, 3, 8)\n",
        "upsample6 = upsampling(conv12)\n",
        "conv13 = convolution(upsample6, 8, 3, 2)\n",
        "loss = tf.losses.mean_squared_error(labels = ytrue, predictions = conv13)\n",
        "cost = tf.reduce_mean(loss)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = 0.0001).minimize(cost)\n",
        "session.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8Ndi5IOzhIlb",
        "outputId": "084baaa0-b787-454d-d4a3-faa04e1f553a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 1500\n",
        "for i in range(num_epochs):\n",
        "    session.run(optimizer, feed_dict = {x: X, ytrue:Y})\n",
        "    lossvalue = session.run(cost, feed_dict = {x:X, ytrue : Y})\n",
        "    print(\"epoch: \" + str(i) + \" loss: \" + str(lossvalue))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 loss: 0.07058012\n",
            "epoch: 1 loss: 0.05880915\n",
            "epoch: 2 loss: 0.049540598\n",
            "epoch: 3 loss: 0.043145966\n",
            "epoch: 4 loss: 0.038944565\n",
            "epoch: 5 loss: 0.035608064\n",
            "epoch: 6 loss: 0.0327073\n",
            "epoch: 7 loss: 0.0304252\n",
            "epoch: 8 loss: 0.028793657\n",
            "epoch: 9 loss: 0.027644327\n",
            "epoch: 10 loss: 0.02680426\n",
            "epoch: 11 loss: 0.026173228\n",
            "epoch: 12 loss: 0.02569477\n",
            "epoch: 13 loss: 0.025318988\n",
            "epoch: 14 loss: 0.024983803\n",
            "epoch: 15 loss: 0.02462787\n",
            "epoch: 16 loss: 0.024223728\n",
            "epoch: 17 loss: 0.023782002\n",
            "epoch: 18 loss: 0.023339478\n",
            "epoch: 19 loss: 0.022935515\n",
            "epoch: 20 loss: 0.022594886\n",
            "epoch: 21 loss: 0.022320686\n",
            "epoch: 22 loss: 0.022097034\n",
            "epoch: 23 loss: 0.021901665\n",
            "epoch: 24 loss: 0.021717349\n",
            "epoch: 25 loss: 0.021538682\n",
            "epoch: 26 loss: 0.021370025\n",
            "epoch: 27 loss: 0.02121815\n",
            "epoch: 28 loss: 0.021083599\n",
            "epoch: 29 loss: 0.020961\n",
            "epoch: 30 loss: 0.02084245\n",
            "epoch: 31 loss: 0.02072337\n",
            "epoch: 32 loss: 0.020604584\n",
            "epoch: 33 loss: 0.020491255\n",
            "epoch: 34 loss: 0.020389153\n",
            "epoch: 35 loss: 0.02030108\n",
            "epoch: 36 loss: 0.020224199\n",
            "epoch: 37 loss: 0.02015232\n",
            "epoch: 38 loss: 0.020079339\n",
            "epoch: 39 loss: 0.020002581\n",
            "epoch: 40 loss: 0.019923585\n",
            "epoch: 41 loss: 0.019846218\n",
            "epoch: 42 loss: 0.019773936\n",
            "epoch: 43 loss: 0.019707892\n",
            "epoch: 44 loss: 0.019646732\n",
            "epoch: 45 loss: 0.019588284\n",
            "epoch: 46 loss: 0.019531146\n",
            "epoch: 47 loss: 0.019475318\n",
            "epoch: 48 loss: 0.019421525\n",
            "epoch: 49 loss: 0.019370278\n",
            "epoch: 50 loss: 0.019321106\n",
            "epoch: 51 loss: 0.019272966\n",
            "epoch: 52 loss: 0.019224936\n",
            "epoch: 53 loss: 0.01917699\n",
            "epoch: 54 loss: 0.019130116\n",
            "epoch: 55 loss: 0.019085387\n",
            "epoch: 56 loss: 0.01904313\n",
            "epoch: 57 loss: 0.01900288\n",
            "epoch: 58 loss: 0.018963667\n",
            "epoch: 59 loss: 0.018924804\n",
            "epoch: 60 loss: 0.018886007\n",
            "epoch: 61 loss: 0.018847652\n",
            "epoch: 62 loss: 0.018810183\n",
            "epoch: 63 loss: 0.018773727\n",
            "epoch: 64 loss: 0.018738115\n",
            "epoch: 65 loss: 0.018703042\n",
            "epoch: 66 loss: 0.018668488\n",
            "epoch: 67 loss: 0.018634614\n",
            "epoch: 68 loss: 0.018601554\n",
            "epoch: 69 loss: 0.018569304\n",
            "epoch: 70 loss: 0.018537614\n",
            "epoch: 71 loss: 0.018506216\n",
            "epoch: 72 loss: 0.018474909\n",
            "epoch: 73 loss: 0.018443843\n",
            "epoch: 74 loss: 0.018413179\n",
            "epoch: 75 loss: 0.018383063\n",
            "epoch: 76 loss: 0.018353485\n",
            "epoch: 77 loss: 0.018324321\n",
            "epoch: 78 loss: 0.018295487\n",
            "epoch: 79 loss: 0.0182669\n",
            "epoch: 80 loss: 0.018238647\n",
            "epoch: 81 loss: 0.018210797\n",
            "epoch: 82 loss: 0.018183246\n",
            "epoch: 83 loss: 0.01815593\n",
            "epoch: 84 loss: 0.018128822\n",
            "epoch: 85 loss: 0.018101912\n",
            "epoch: 86 loss: 0.01807529\n",
            "epoch: 87 loss: 0.01804904\n",
            "epoch: 88 loss: 0.018023074\n",
            "epoch: 89 loss: 0.017997285\n",
            "epoch: 90 loss: 0.017971592\n",
            "epoch: 91 loss: 0.017946\n",
            "epoch: 92 loss: 0.017920537\n",
            "epoch: 93 loss: 0.017895246\n",
            "epoch: 94 loss: 0.017870104\n",
            "epoch: 95 loss: 0.017845014\n",
            "epoch: 96 loss: 0.017819956\n",
            "epoch: 97 loss: 0.017794965\n",
            "epoch: 98 loss: 0.017770013\n",
            "epoch: 99 loss: 0.017745117\n",
            "epoch: 100 loss: 0.017720236\n",
            "epoch: 101 loss: 0.017695319\n",
            "epoch: 102 loss: 0.01767041\n",
            "epoch: 103 loss: 0.017645482\n",
            "epoch: 104 loss: 0.017620487\n",
            "epoch: 105 loss: 0.01759549\n",
            "epoch: 106 loss: 0.017570503\n",
            "epoch: 107 loss: 0.017545478\n",
            "epoch: 108 loss: 0.01752039\n",
            "epoch: 109 loss: 0.017495183\n",
            "epoch: 110 loss: 0.017469881\n",
            "epoch: 111 loss: 0.01744447\n",
            "epoch: 112 loss: 0.017418955\n",
            "epoch: 113 loss: 0.017393308\n",
            "epoch: 114 loss: 0.017367521\n",
            "epoch: 115 loss: 0.017341549\n",
            "epoch: 116 loss: 0.017315404\n",
            "epoch: 117 loss: 0.017289104\n",
            "epoch: 118 loss: 0.017262662\n",
            "epoch: 119 loss: 0.017236013\n",
            "epoch: 120 loss: 0.017209152\n",
            "epoch: 121 loss: 0.017182203\n",
            "epoch: 122 loss: 0.017155098\n",
            "epoch: 123 loss: 0.017127844\n",
            "epoch: 124 loss: 0.017100444\n",
            "epoch: 125 loss: 0.01707282\n",
            "epoch: 126 loss: 0.01704497\n",
            "epoch: 127 loss: 0.01701693\n",
            "epoch: 128 loss: 0.016988622\n",
            "epoch: 129 loss: 0.016960032\n",
            "epoch: 130 loss: 0.016931191\n",
            "epoch: 131 loss: 0.016902063\n",
            "epoch: 132 loss: 0.016872643\n",
            "epoch: 133 loss: 0.016842945\n",
            "epoch: 134 loss: 0.016812915\n",
            "epoch: 135 loss: 0.016782546\n",
            "epoch: 136 loss: 0.016751913\n",
            "epoch: 137 loss: 0.016721012\n",
            "epoch: 138 loss: 0.016689783\n",
            "epoch: 139 loss: 0.016658248\n",
            "epoch: 140 loss: 0.01662638\n",
            "epoch: 141 loss: 0.016594185\n",
            "epoch: 142 loss: 0.016561622\n",
            "epoch: 143 loss: 0.016528673\n",
            "epoch: 144 loss: 0.016495364\n",
            "epoch: 145 loss: 0.016461818\n",
            "epoch: 146 loss: 0.016428042\n",
            "epoch: 147 loss: 0.016393842\n",
            "epoch: 148 loss: 0.016359186\n",
            "epoch: 149 loss: 0.01632411\n",
            "epoch: 150 loss: 0.016288659\n",
            "epoch: 151 loss: 0.016252793\n",
            "epoch: 152 loss: 0.016216459\n",
            "epoch: 153 loss: 0.016179675\n",
            "epoch: 154 loss: 0.016142443\n",
            "epoch: 155 loss: 0.016104769\n",
            "epoch: 156 loss: 0.016066765\n",
            "epoch: 157 loss: 0.01602834\n",
            "epoch: 158 loss: 0.015989592\n",
            "epoch: 159 loss: 0.015950527\n",
            "epoch: 160 loss: 0.015911158\n",
            "epoch: 161 loss: 0.015871435\n",
            "epoch: 162 loss: 0.015831405\n",
            "epoch: 163 loss: 0.015790986\n",
            "epoch: 164 loss: 0.015750203\n",
            "epoch: 165 loss: 0.015709134\n",
            "epoch: 166 loss: 0.01566776\n",
            "epoch: 167 loss: 0.015626067\n",
            "epoch: 168 loss: 0.015584078\n",
            "epoch: 169 loss: 0.015541787\n",
            "epoch: 170 loss: 0.015499111\n",
            "epoch: 171 loss: 0.015456109\n",
            "epoch: 172 loss: 0.015412721\n",
            "epoch: 173 loss: 0.015368946\n",
            "epoch: 174 loss: 0.015324816\n",
            "epoch: 175 loss: 0.015280341\n",
            "epoch: 176 loss: 0.015235505\n",
            "epoch: 177 loss: 0.01519043\n",
            "epoch: 178 loss: 0.015145057\n",
            "epoch: 179 loss: 0.015099291\n",
            "epoch: 180 loss: 0.015053215\n",
            "epoch: 181 loss: 0.015006825\n",
            "epoch: 182 loss: 0.014960156\n",
            "epoch: 183 loss: 0.014913275\n",
            "epoch: 184 loss: 0.014866111\n",
            "epoch: 185 loss: 0.014818768\n",
            "epoch: 186 loss: 0.014771182\n",
            "epoch: 187 loss: 0.0147234155\n",
            "epoch: 188 loss: 0.014675415\n",
            "epoch: 189 loss: 0.014627163\n",
            "epoch: 190 loss: 0.014578701\n",
            "epoch: 191 loss: 0.01452998\n",
            "epoch: 192 loss: 0.014481101\n",
            "epoch: 193 loss: 0.014431982\n",
            "epoch: 194 loss: 0.014382574\n",
            "epoch: 195 loss: 0.014332917\n",
            "epoch: 196 loss: 0.014283202\n",
            "epoch: 197 loss: 0.014233381\n",
            "epoch: 198 loss: 0.014183267\n",
            "epoch: 199 loss: 0.014133024\n",
            "epoch: 200 loss: 0.014082524\n",
            "epoch: 201 loss: 0.0140318265\n",
            "epoch: 202 loss: 0.013980974\n",
            "epoch: 203 loss: 0.013929918\n",
            "epoch: 204 loss: 0.0138786845\n",
            "epoch: 205 loss: 0.013827261\n",
            "epoch: 206 loss: 0.013775603\n",
            "epoch: 207 loss: 0.013724025\n",
            "epoch: 208 loss: 0.013672336\n",
            "epoch: 209 loss: 0.013620391\n",
            "epoch: 210 loss: 0.013568289\n",
            "epoch: 211 loss: 0.013516081\n",
            "epoch: 212 loss: 0.013463744\n",
            "epoch: 213 loss: 0.0134113245\n",
            "epoch: 214 loss: 0.013358933\n",
            "epoch: 215 loss: 0.013306583\n",
            "epoch: 216 loss: 0.013254113\n",
            "epoch: 217 loss: 0.013201486\n",
            "epoch: 218 loss: 0.01314884\n",
            "epoch: 219 loss: 0.013096183\n",
            "epoch: 220 loss: 0.013043415\n",
            "epoch: 221 loss: 0.012990692\n",
            "epoch: 222 loss: 0.012938168\n",
            "epoch: 223 loss: 0.012885594\n",
            "epoch: 224 loss: 0.012832969\n",
            "epoch: 225 loss: 0.012780424\n",
            "epoch: 226 loss: 0.0127281025\n",
            "epoch: 227 loss: 0.01267594\n",
            "epoch: 228 loss: 0.01262403\n",
            "epoch: 229 loss: 0.012572356\n",
            "epoch: 230 loss: 0.012520765\n",
            "epoch: 231 loss: 0.01246933\n",
            "epoch: 232 loss: 0.0124181835\n",
            "epoch: 233 loss: 0.012367405\n",
            "epoch: 234 loss: 0.012317029\n",
            "epoch: 235 loss: 0.0122670075\n",
            "epoch: 236 loss: 0.012217332\n",
            "epoch: 237 loss: 0.012168008\n",
            "epoch: 238 loss: 0.012119231\n",
            "epoch: 239 loss: 0.01207092\n",
            "epoch: 240 loss: 0.01202296\n",
            "epoch: 241 loss: 0.0119754085\n",
            "epoch: 242 loss: 0.011928261\n",
            "epoch: 243 loss: 0.011881679\n",
            "epoch: 244 loss: 0.011835649\n",
            "epoch: 245 loss: 0.011790206\n",
            "epoch: 246 loss: 0.011745169\n",
            "epoch: 247 loss: 0.0117006935\n",
            "epoch: 248 loss: 0.011656697\n",
            "epoch: 249 loss: 0.011613213\n",
            "epoch: 250 loss: 0.011570158\n",
            "epoch: 251 loss: 0.011527588\n",
            "epoch: 252 loss: 0.011485477\n",
            "epoch: 253 loss: 0.011443843\n",
            "epoch: 254 loss: 0.011402823\n",
            "epoch: 255 loss: 0.011362364\n",
            "epoch: 256 loss: 0.01132232\n",
            "epoch: 257 loss: 0.011282868\n",
            "epoch: 258 loss: 0.011243951\n",
            "epoch: 259 loss: 0.01120541\n",
            "epoch: 260 loss: 0.011167265\n",
            "epoch: 261 loss: 0.0111296475\n",
            "epoch: 262 loss: 0.011092448\n",
            "epoch: 263 loss: 0.01105569\n",
            "epoch: 264 loss: 0.011019382\n",
            "epoch: 265 loss: 0.010983496\n",
            "epoch: 266 loss: 0.010948088\n",
            "epoch: 267 loss: 0.010913059\n",
            "epoch: 268 loss: 0.010878414\n",
            "epoch: 269 loss: 0.010844252\n",
            "epoch: 270 loss: 0.010810451\n",
            "epoch: 271 loss: 0.010777173\n",
            "epoch: 272 loss: 0.010744323\n",
            "epoch: 273 loss: 0.010711901\n",
            "epoch: 274 loss: 0.010679874\n",
            "epoch: 275 loss: 0.010648206\n",
            "epoch: 276 loss: 0.010616901\n",
            "epoch: 277 loss: 0.010585998\n",
            "epoch: 278 loss: 0.010555425\n",
            "epoch: 279 loss: 0.010525222\n",
            "epoch: 280 loss: 0.010495383\n",
            "epoch: 281 loss: 0.010465954\n",
            "epoch: 282 loss: 0.010436859\n",
            "epoch: 283 loss: 0.010408058\n",
            "epoch: 284 loss: 0.0103795845\n",
            "epoch: 285 loss: 0.01035142\n",
            "epoch: 286 loss: 0.010323542\n",
            "epoch: 287 loss: 0.010295964\n",
            "epoch: 288 loss: 0.010268695\n",
            "epoch: 289 loss: 0.010241788\n",
            "epoch: 290 loss: 0.010215196\n",
            "epoch: 291 loss: 0.010188868\n",
            "epoch: 292 loss: 0.010162823\n",
            "epoch: 293 loss: 0.010137039\n",
            "epoch: 294 loss: 0.0101115415\n",
            "epoch: 295 loss: 0.010086249\n",
            "epoch: 296 loss: 0.010061251\n",
            "epoch: 297 loss: 0.0100364955\n",
            "epoch: 298 loss: 0.010011989\n",
            "epoch: 299 loss: 0.009987787\n",
            "epoch: 300 loss: 0.009963843\n",
            "epoch: 301 loss: 0.009940143\n",
            "epoch: 302 loss: 0.0099166585\n",
            "epoch: 303 loss: 0.009893416\n",
            "epoch: 304 loss: 0.00987037\n",
            "epoch: 305 loss: 0.009847538\n",
            "epoch: 306 loss: 0.009824897\n",
            "epoch: 307 loss: 0.009802491\n",
            "epoch: 308 loss: 0.0097802775\n",
            "epoch: 309 loss: 0.009758259\n",
            "epoch: 310 loss: 0.009736462\n",
            "epoch: 311 loss: 0.009714921\n",
            "epoch: 312 loss: 0.009693597\n",
            "epoch: 313 loss: 0.0096724415\n",
            "epoch: 314 loss: 0.009651492\n",
            "epoch: 315 loss: 0.009630744\n",
            "epoch: 316 loss: 0.0096101845\n",
            "epoch: 317 loss: 0.00958979\n",
            "epoch: 318 loss: 0.009569581\n",
            "epoch: 319 loss: 0.009549529\n",
            "epoch: 320 loss: 0.009529627\n",
            "epoch: 321 loss: 0.009509933\n",
            "epoch: 322 loss: 0.009490416\n",
            "epoch: 323 loss: 0.00947107\n",
            "epoch: 324 loss: 0.00945187\n",
            "epoch: 325 loss: 0.009432848\n",
            "epoch: 326 loss: 0.009414014\n",
            "epoch: 327 loss: 0.009395331\n",
            "epoch: 328 loss: 0.009376795\n",
            "epoch: 329 loss: 0.009358408\n",
            "epoch: 330 loss: 0.009340192\n",
            "epoch: 331 loss: 0.0093221385\n",
            "epoch: 332 loss: 0.009304201\n",
            "epoch: 333 loss: 0.009286373\n",
            "epoch: 334 loss: 0.0092687085\n",
            "epoch: 335 loss: 0.009251177\n",
            "epoch: 336 loss: 0.009233792\n",
            "epoch: 337 loss: 0.009216576\n",
            "epoch: 338 loss: 0.009199491\n",
            "epoch: 339 loss: 0.0091825435\n",
            "epoch: 340 loss: 0.009165712\n",
            "epoch: 341 loss: 0.009149013\n",
            "epoch: 342 loss: 0.00913244\n",
            "epoch: 343 loss: 0.009116006\n",
            "epoch: 344 loss: 0.009099689\n",
            "epoch: 345 loss: 0.009083501\n",
            "epoch: 346 loss: 0.009067419\n",
            "epoch: 347 loss: 0.009051459\n",
            "epoch: 348 loss: 0.009035602\n",
            "epoch: 349 loss: 0.009019842\n",
            "epoch: 350 loss: 0.009004227\n",
            "epoch: 351 loss: 0.008988691\n",
            "epoch: 352 loss: 0.008973289\n",
            "epoch: 353 loss: 0.008957985\n",
            "epoch: 354 loss: 0.008942762\n",
            "epoch: 355 loss: 0.008927658\n",
            "epoch: 356 loss: 0.008912653\n",
            "epoch: 357 loss: 0.008897757\n",
            "epoch: 358 loss: 0.008882958\n",
            "epoch: 359 loss: 0.0088682715\n",
            "epoch: 360 loss: 0.00885368\n",
            "epoch: 361 loss: 0.008839195\n",
            "epoch: 362 loss: 0.008824806\n",
            "epoch: 363 loss: 0.008810528\n",
            "epoch: 364 loss: 0.008796375\n",
            "epoch: 365 loss: 0.008782305\n",
            "epoch: 366 loss: 0.008768315\n",
            "epoch: 367 loss: 0.008754411\n",
            "epoch: 368 loss: 0.008740609\n",
            "epoch: 369 loss: 0.008726921\n",
            "epoch: 370 loss: 0.008713325\n",
            "epoch: 371 loss: 0.008699827\n",
            "epoch: 372 loss: 0.008686422\n",
            "epoch: 373 loss: 0.008673097\n",
            "epoch: 374 loss: 0.008659846\n",
            "epoch: 375 loss: 0.008646693\n",
            "epoch: 376 loss: 0.008633644\n",
            "epoch: 377 loss: 0.008620673\n",
            "epoch: 378 loss: 0.008607768\n",
            "epoch: 379 loss: 0.008594963\n",
            "epoch: 380 loss: 0.008582249\n",
            "epoch: 381 loss: 0.008569607\n",
            "epoch: 382 loss: 0.008557065\n",
            "epoch: 383 loss: 0.008544596\n",
            "epoch: 384 loss: 0.008532183\n",
            "epoch: 385 loss: 0.0085198525\n",
            "epoch: 386 loss: 0.008507606\n",
            "epoch: 387 loss: 0.008495435\n",
            "epoch: 388 loss: 0.008483353\n",
            "epoch: 389 loss: 0.008471348\n",
            "epoch: 390 loss: 0.008459416\n",
            "epoch: 391 loss: 0.008447543\n",
            "epoch: 392 loss: 0.008435752\n",
            "epoch: 393 loss: 0.008424038\n",
            "epoch: 394 loss: 0.008412402\n",
            "epoch: 395 loss: 0.008400821\n",
            "epoch: 396 loss: 0.008389312\n",
            "epoch: 397 loss: 0.008377868\n",
            "epoch: 398 loss: 0.008366495\n",
            "epoch: 399 loss: 0.008355189\n",
            "epoch: 400 loss: 0.008343961\n",
            "epoch: 401 loss: 0.008332803\n",
            "epoch: 402 loss: 0.008321697\n",
            "epoch: 403 loss: 0.008310646\n",
            "epoch: 404 loss: 0.008299666\n",
            "epoch: 405 loss: 0.008288748\n",
            "epoch: 406 loss: 0.008277889\n",
            "epoch: 407 loss: 0.0082670795\n",
            "epoch: 408 loss: 0.008256351\n",
            "epoch: 409 loss: 0.008245686\n",
            "epoch: 410 loss: 0.008235063\n",
            "epoch: 411 loss: 0.008224504\n",
            "epoch: 412 loss: 0.008214001\n",
            "epoch: 413 loss: 0.008203542\n",
            "epoch: 414 loss: 0.00819314\n",
            "epoch: 415 loss: 0.008182785\n",
            "epoch: 416 loss: 0.008172486\n",
            "epoch: 417 loss: 0.008162225\n",
            "epoch: 418 loss: 0.008152021\n",
            "epoch: 419 loss: 0.00814187\n",
            "epoch: 420 loss: 0.008131764\n",
            "epoch: 421 loss: 0.008121702\n",
            "epoch: 422 loss: 0.008111697\n",
            "epoch: 423 loss: 0.008101733\n",
            "epoch: 424 loss: 0.008091828\n",
            "epoch: 425 loss: 0.008081966\n",
            "epoch: 426 loss: 0.008072142\n",
            "epoch: 427 loss: 0.008062368\n",
            "epoch: 428 loss: 0.008052633\n",
            "epoch: 429 loss: 0.008042946\n",
            "epoch: 430 loss: 0.008033302\n",
            "epoch: 431 loss: 0.008023693\n",
            "epoch: 432 loss: 0.008014149\n",
            "epoch: 433 loss: 0.008004643\n",
            "epoch: 434 loss: 0.0079951845\n",
            "epoch: 435 loss: 0.007985764\n",
            "epoch: 436 loss: 0.007976375\n",
            "epoch: 437 loss: 0.007967042\n",
            "epoch: 438 loss: 0.007957752\n",
            "epoch: 439 loss: 0.00794849\n",
            "epoch: 440 loss: 0.007939278\n",
            "epoch: 441 loss: 0.007930124\n",
            "epoch: 442 loss: 0.007920994\n",
            "epoch: 443 loss: 0.007911898\n",
            "epoch: 444 loss: 0.007902862\n",
            "epoch: 445 loss: 0.007893845\n",
            "epoch: 446 loss: 0.0078848805\n",
            "epoch: 447 loss: 0.007875948\n",
            "epoch: 448 loss: 0.0078670615\n",
            "epoch: 449 loss: 0.007858201\n",
            "epoch: 450 loss: 0.00784938\n",
            "epoch: 451 loss: 0.007840611\n",
            "epoch: 452 loss: 0.007831865\n",
            "epoch: 453 loss: 0.007823148\n",
            "epoch: 454 loss: 0.007814468\n",
            "epoch: 455 loss: 0.0078058275\n",
            "epoch: 456 loss: 0.007797208\n",
            "epoch: 457 loss: 0.0077886325\n",
            "epoch: 458 loss: 0.0077800844\n",
            "epoch: 459 loss: 0.007771553\n",
            "epoch: 460 loss: 0.0077630705\n",
            "epoch: 461 loss: 0.0077546104\n",
            "epoch: 462 loss: 0.007746178\n",
            "epoch: 463 loss: 0.007737788\n",
            "epoch: 464 loss: 0.0077294335\n",
            "epoch: 465 loss: 0.007721113\n",
            "epoch: 466 loss: 0.007712822\n",
            "epoch: 467 loss: 0.007704561\n",
            "epoch: 468 loss: 0.007696327\n",
            "epoch: 469 loss: 0.0076881177\n",
            "epoch: 470 loss: 0.0076799393\n",
            "epoch: 471 loss: 0.007671784\n",
            "epoch: 472 loss: 0.007663654\n",
            "epoch: 473 loss: 0.0076555493\n",
            "epoch: 474 loss: 0.0076474645\n",
            "epoch: 475 loss: 0.0076394104\n",
            "epoch: 476 loss: 0.007631389\n",
            "epoch: 477 loss: 0.0076233908\n",
            "epoch: 478 loss: 0.0076154196\n",
            "epoch: 479 loss: 0.007607477\n",
            "epoch: 480 loss: 0.007599566\n",
            "epoch: 481 loss: 0.007591681\n",
            "epoch: 482 loss: 0.00758382\n",
            "epoch: 483 loss: 0.0075759855\n",
            "epoch: 484 loss: 0.0075681754\n",
            "epoch: 485 loss: 0.007560386\n",
            "epoch: 486 loss: 0.007552627\n",
            "epoch: 487 loss: 0.0075448956\n",
            "epoch: 488 loss: 0.0075371847\n",
            "epoch: 489 loss: 0.0075295027\n",
            "epoch: 490 loss: 0.0075218496\n",
            "epoch: 491 loss: 0.0075142267\n",
            "epoch: 492 loss: 0.0075066327\n",
            "epoch: 493 loss: 0.007499059\n",
            "epoch: 494 loss: 0.00749151\n",
            "epoch: 495 loss: 0.0074839764\n",
            "epoch: 496 loss: 0.0074764825\n",
            "epoch: 497 loss: 0.0074690017\n",
            "epoch: 498 loss: 0.007461541\n",
            "epoch: 499 loss: 0.007454122\n",
            "epoch: 500 loss: 0.007446721\n",
            "epoch: 501 loss: 0.0074393502\n",
            "epoch: 502 loss: 0.0074319984\n",
            "epoch: 503 loss: 0.0074246726\n",
            "epoch: 504 loss: 0.0074173636\n",
            "epoch: 505 loss: 0.007410077\n",
            "epoch: 506 loss: 0.00740281\n",
            "epoch: 507 loss: 0.007395562\n",
            "epoch: 508 loss: 0.007388338\n",
            "epoch: 509 loss: 0.00738114\n",
            "epoch: 510 loss: 0.007373957\n",
            "epoch: 511 loss: 0.007366795\n",
            "epoch: 512 loss: 0.007359658\n",
            "epoch: 513 loss: 0.007352539\n",
            "epoch: 514 loss: 0.0073454455\n",
            "epoch: 515 loss: 0.0073383674\n",
            "epoch: 516 loss: 0.007331315\n",
            "epoch: 517 loss: 0.0073242816\n",
            "epoch: 518 loss: 0.007317275\n",
            "epoch: 519 loss: 0.0073102806\n",
            "epoch: 520 loss: 0.0073033166\n",
            "epoch: 521 loss: 0.0072963643\n",
            "epoch: 522 loss: 0.0072894394\n",
            "epoch: 523 loss: 0.0072825286\n",
            "epoch: 524 loss: 0.0072756414\n",
            "epoch: 525 loss: 0.00726877\n",
            "epoch: 526 loss: 0.0072619277\n",
            "epoch: 527 loss: 0.0072550946\n",
            "epoch: 528 loss: 0.0072482857\n",
            "epoch: 529 loss: 0.007241498\n",
            "epoch: 530 loss: 0.007234728\n",
            "epoch: 531 loss: 0.0072279754\n",
            "epoch: 532 loss: 0.007221248\n",
            "epoch: 533 loss: 0.0072145355\n",
            "epoch: 534 loss: 0.0072078425\n",
            "epoch: 535 loss: 0.0072011687\n",
            "epoch: 536 loss: 0.0071945097\n",
            "epoch: 537 loss: 0.007187867\n",
            "epoch: 538 loss: 0.007181235\n",
            "epoch: 539 loss: 0.007174626\n",
            "epoch: 540 loss: 0.0071680285\n",
            "epoch: 541 loss: 0.0071614496\n",
            "epoch: 542 loss: 0.007154889\n",
            "epoch: 543 loss: 0.007148339\n",
            "epoch: 544 loss: 0.007141808\n",
            "epoch: 545 loss: 0.007135296\n",
            "epoch: 546 loss: 0.007128801\n",
            "epoch: 547 loss: 0.0071223252\n",
            "epoch: 548 loss: 0.007115869\n",
            "epoch: 549 loss: 0.007109424\n",
            "epoch: 550 loss: 0.007103004\n",
            "epoch: 551 loss: 0.007096604\n",
            "epoch: 552 loss: 0.0070902132\n",
            "epoch: 553 loss: 0.0070838467\n",
            "epoch: 554 loss: 0.007077498\n",
            "epoch: 555 loss: 0.007071165\n",
            "epoch: 556 loss: 0.0070648473\n",
            "epoch: 557 loss: 0.00705854\n",
            "epoch: 558 loss: 0.007052249\n",
            "epoch: 559 loss: 0.007045969\n",
            "epoch: 560 loss: 0.007039704\n",
            "epoch: 561 loss: 0.0070334575\n",
            "epoch: 562 loss: 0.007027224\n",
            "epoch: 563 loss: 0.00702101\n",
            "epoch: 564 loss: 0.0070148124\n",
            "epoch: 565 loss: 0.007008624\n",
            "epoch: 566 loss: 0.00700246\n",
            "epoch: 567 loss: 0.006996295\n",
            "epoch: 568 loss: 0.0069901645\n",
            "epoch: 569 loss: 0.006984031\n",
            "epoch: 570 loss: 0.006977916\n",
            "epoch: 571 loss: 0.0069718226\n",
            "epoch: 572 loss: 0.006965744\n",
            "epoch: 573 loss: 0.0069596763\n",
            "epoch: 574 loss: 0.006953618\n",
            "epoch: 575 loss: 0.0069475747\n",
            "epoch: 576 loss: 0.006941547\n",
            "epoch: 577 loss: 0.006935528\n",
            "epoch: 578 loss: 0.006929531\n",
            "epoch: 579 loss: 0.0069235493\n",
            "epoch: 580 loss: 0.006917584\n",
            "epoch: 581 loss: 0.0069116373\n",
            "epoch: 582 loss: 0.006905707\n",
            "epoch: 583 loss: 0.006899783\n",
            "epoch: 584 loss: 0.006893879\n",
            "epoch: 585 loss: 0.0068879873\n",
            "epoch: 586 loss: 0.0068821083\n",
            "epoch: 587 loss: 0.0068762437\n",
            "epoch: 588 loss: 0.0068703876\n",
            "epoch: 589 loss: 0.006864549\n",
            "epoch: 590 loss: 0.006858725\n",
            "epoch: 591 loss: 0.006852914\n",
            "epoch: 592 loss: 0.006847113\n",
            "epoch: 593 loss: 0.0068413285\n",
            "epoch: 594 loss: 0.006835558\n",
            "epoch: 595 loss: 0.0068298033\n",
            "epoch: 596 loss: 0.0068240575\n",
            "epoch: 597 loss: 0.0068183243\n",
            "epoch: 598 loss: 0.0068126023\n",
            "epoch: 599 loss: 0.006806893\n",
            "epoch: 600 loss: 0.006801196\n",
            "epoch: 601 loss: 0.006795515\n",
            "epoch: 602 loss: 0.0067898426\n",
            "epoch: 603 loss: 0.0067841797\n",
            "epoch: 604 loss: 0.0067785415\n",
            "epoch: 605 loss: 0.0067728995\n",
            "epoch: 606 loss: 0.006767282\n",
            "epoch: 607 loss: 0.0067616687\n",
            "epoch: 608 loss: 0.006756071\n",
            "epoch: 609 loss: 0.0067504793\n",
            "epoch: 610 loss: 0.00674491\n",
            "epoch: 611 loss: 0.006739352\n",
            "epoch: 612 loss: 0.0067337994\n",
            "epoch: 613 loss: 0.006728247\n",
            "epoch: 614 loss: 0.006722705\n",
            "epoch: 615 loss: 0.006717172\n",
            "epoch: 616 loss: 0.006711649\n",
            "epoch: 617 loss: 0.006706139\n",
            "epoch: 618 loss: 0.0067006326\n",
            "epoch: 619 loss: 0.0066951416\n",
            "epoch: 620 loss: 0.0066896635\n",
            "epoch: 621 loss: 0.006684193\n",
            "epoch: 622 loss: 0.0066787396\n",
            "epoch: 623 loss: 0.006673293\n",
            "epoch: 624 loss: 0.006667867\n",
            "epoch: 625 loss: 0.006662441\n",
            "epoch: 626 loss: 0.0066570383\n",
            "epoch: 627 loss: 0.006651644\n",
            "epoch: 628 loss: 0.0066462546\n",
            "epoch: 629 loss: 0.00664088\n",
            "epoch: 630 loss: 0.0066355173\n",
            "epoch: 631 loss: 0.0066301636\n",
            "epoch: 632 loss: 0.0066248216\n",
            "epoch: 633 loss: 0.0066194893\n",
            "epoch: 634 loss: 0.0066141747\n",
            "epoch: 635 loss: 0.0066088606\n",
            "epoch: 636 loss: 0.0066035558\n",
            "epoch: 637 loss: 0.0065982654\n",
            "epoch: 638 loss: 0.006592983\n",
            "epoch: 639 loss: 0.0065877046\n",
            "epoch: 640 loss: 0.0065824403\n",
            "epoch: 641 loss: 0.006577183\n",
            "epoch: 642 loss: 0.006571938\n",
            "epoch: 643 loss: 0.0065667\n",
            "epoch: 644 loss: 0.0065614767\n",
            "epoch: 645 loss: 0.0065562665\n",
            "epoch: 646 loss: 0.006551065\n",
            "epoch: 647 loss: 0.0065458696\n",
            "epoch: 648 loss: 0.0065406803\n",
            "epoch: 649 loss: 0.0065355133\n",
            "epoch: 650 loss: 0.0065303524\n",
            "epoch: 651 loss: 0.0065251994\n",
            "epoch: 652 loss: 0.0065200743\n",
            "epoch: 653 loss: 0.0065149665\n",
            "epoch: 654 loss: 0.006509872\n",
            "epoch: 655 loss: 0.006504795\n",
            "epoch: 656 loss: 0.006499728\n",
            "epoch: 657 loss: 0.006494696\n",
            "epoch: 658 loss: 0.0064896983\n",
            "epoch: 659 loss: 0.0064847576\n",
            "epoch: 660 loss: 0.0064798878\n",
            "epoch: 661 loss: 0.0064751063\n",
            "epoch: 662 loss: 0.006470453\n",
            "epoch: 663 loss: 0.006466016\n",
            "epoch: 664 loss: 0.0064618993\n",
            "epoch: 665 loss: 0.0064581665\n",
            "epoch: 666 loss: 0.0064549544\n",
            "epoch: 667 loss: 0.006452368\n",
            "epoch: 668 loss: 0.006450286\n",
            "epoch: 669 loss: 0.0064483923\n",
            "epoch: 670 loss: 0.006445456\n",
            "epoch: 671 loss: 0.006440848\n",
            "epoch: 672 loss: 0.0064332825\n",
            "epoch: 673 loss: 0.0064236843\n",
            "epoch: 674 loss: 0.0064137564\n",
            "epoch: 675 loss: 0.00640576\n",
            "epoch: 676 loss: 0.0064006094\n",
            "epoch: 677 loss: 0.00639768\n",
            "epoch: 678 loss: 0.0063954988\n",
            "epoch: 679 loss: 0.0063925586\n",
            "epoch: 680 loss: 0.006388078\n",
            "epoch: 681 loss: 0.0063819718\n",
            "epoch: 682 loss: 0.0063749887\n",
            "epoch: 683 loss: 0.00636816\n",
            "epoch: 684 loss: 0.0063623064\n",
            "epoch: 685 loss: 0.006357631\n",
            "epoch: 686 loss: 0.0063538016\n",
            "epoch: 687 loss: 0.0063502416\n",
            "epoch: 688 loss: 0.0063464027\n",
            "epoch: 689 loss: 0.006341956\n",
            "epoch: 690 loss: 0.0063369186\n",
            "epoch: 691 loss: 0.006331455\n",
            "epoch: 692 loss: 0.0063258763\n",
            "epoch: 693 loss: 0.0063204668\n",
            "epoch: 694 loss: 0.0063154246\n",
            "epoch: 695 loss: 0.006310782\n",
            "epoch: 696 loss: 0.0063064396\n",
            "epoch: 697 loss: 0.006302249\n",
            "epoch: 698 loss: 0.0062980694\n",
            "epoch: 699 loss: 0.0062938035\n",
            "epoch: 700 loss: 0.0062893895\n",
            "epoch: 701 loss: 0.0062848073\n",
            "epoch: 702 loss: 0.006280093\n",
            "epoch: 703 loss: 0.006275278\n",
            "epoch: 704 loss: 0.0062704342\n",
            "epoch: 705 loss: 0.006265588\n",
            "epoch: 706 loss: 0.006260795\n",
            "epoch: 707 loss: 0.0062560863\n",
            "epoch: 708 loss: 0.0062514637\n",
            "epoch: 709 loss: 0.006246923\n",
            "epoch: 710 loss: 0.0062424433\n",
            "epoch: 711 loss: 0.0062380205\n",
            "epoch: 712 loss: 0.006233646\n",
            "epoch: 713 loss: 0.0062293173\n",
            "epoch: 714 loss: 0.0062250253\n",
            "epoch: 715 loss: 0.0062207957\n",
            "epoch: 716 loss: 0.0062166806\n",
            "epoch: 717 loss: 0.0062127043\n",
            "epoch: 718 loss: 0.006208902\n",
            "epoch: 719 loss: 0.006205287\n",
            "epoch: 720 loss: 0.0062019024\n",
            "epoch: 721 loss: 0.0061988668\n",
            "epoch: 722 loss: 0.006196191\n",
            "epoch: 723 loss: 0.0061940276\n",
            "epoch: 724 loss: 0.0061921054\n",
            "epoch: 725 loss: 0.006190403\n",
            "epoch: 726 loss: 0.0061879973\n",
            "epoch: 727 loss: 0.006184582\n",
            "epoch: 728 loss: 0.006178938\n",
            "epoch: 729 loss: 0.0061715203\n",
            "epoch: 730 loss: 0.0061626504\n",
            "epoch: 731 loss: 0.006153995\n",
            "epoch: 732 loss: 0.0061467984\n",
            "epoch: 733 loss: 0.0061417203\n",
            "epoch: 734 loss: 0.0061384924\n",
            "epoch: 735 loss: 0.006136281\n",
            "epoch: 736 loss: 0.006134233\n",
            "epoch: 737 loss: 0.0061315107\n",
            "epoch: 738 loss: 0.0061278697\n",
            "epoch: 739 loss: 0.0061230636\n",
            "epoch: 740 loss: 0.0061174477\n",
            "epoch: 741 loss: 0.006111355\n",
            "epoch: 742 loss: 0.006105418\n",
            "epoch: 743 loss: 0.006100002\n",
            "epoch: 744 loss: 0.0060953354\n",
            "epoch: 745 loss: 0.0060913586\n",
            "epoch: 746 loss: 0.006087836\n",
            "epoch: 747 loss: 0.006084511\n",
            "epoch: 748 loss: 0.0060811397\n",
            "epoch: 749 loss: 0.0060775857\n",
            "epoch: 750 loss: 0.006073719\n",
            "epoch: 751 loss: 0.006069571\n",
            "epoch: 752 loss: 0.0060651377\n",
            "epoch: 753 loss: 0.0060605668\n",
            "epoch: 754 loss: 0.0060558855\n",
            "epoch: 755 loss: 0.006051226\n",
            "epoch: 756 loss: 0.0060466477\n",
            "epoch: 757 loss: 0.006042209\n",
            "epoch: 758 loss: 0.006037907\n",
            "epoch: 759 loss: 0.006033719\n",
            "epoch: 760 loss: 0.006029621\n",
            "epoch: 761 loss: 0.006025592\n",
            "epoch: 762 loss: 0.00602161\n",
            "epoch: 763 loss: 0.006017675\n",
            "epoch: 764 loss: 0.00601379\n",
            "epoch: 765 loss: 0.006009962\n",
            "epoch: 766 loss: 0.0060062236\n",
            "epoch: 767 loss: 0.0060026064\n",
            "epoch: 768 loss: 0.0059991702\n",
            "epoch: 769 loss: 0.0059959893\n",
            "epoch: 770 loss: 0.005993187\n",
            "epoch: 771 loss: 0.0059908344\n",
            "epoch: 772 loss: 0.005989192\n",
            "epoch: 773 loss: 0.0059883557\n",
            "epoch: 774 loss: 0.005988576\n",
            "epoch: 775 loss: 0.0059894673\n",
            "epoch: 776 loss: 0.005990772\n",
            "epoch: 777 loss: 0.0059902756\n",
            "epoch: 778 loss: 0.005987238\n",
            "epoch: 779 loss: 0.005979027\n",
            "epoch: 780 loss: 0.0059673884\n",
            "epoch: 781 loss: 0.005954392\n",
            "epoch: 782 loss: 0.0059442855\n",
            "epoch: 783 loss: 0.0059390035\n",
            "epoch: 784 loss: 0.005937945\n",
            "epoch: 785 loss: 0.0059386645\n",
            "epoch: 786 loss: 0.0059384033\n",
            "epoch: 787 loss: 0.005935689\n",
            "epoch: 788 loss: 0.005929848\n",
            "epoch: 789 loss: 0.0059223496\n",
            "epoch: 790 loss: 0.0059147687\n",
            "epoch: 791 loss: 0.005908761\n",
            "epoch: 792 loss: 0.005904851\n",
            "epoch: 793 loss: 0.0059025586\n",
            "epoch: 794 loss: 0.0059008454\n",
            "epoch: 795 loss: 0.0058986605\n",
            "epoch: 796 loss: 0.0058954093\n",
            "epoch: 797 loss: 0.0058909436\n",
            "epoch: 798 loss: 0.0058857095\n",
            "epoch: 799 loss: 0.005880274\n",
            "epoch: 800 loss: 0.0058752797\n",
            "epoch: 801 loss: 0.0058710338\n",
            "epoch: 802 loss: 0.0058675073\n",
            "epoch: 803 loss: 0.0058644447\n",
            "epoch: 804 loss: 0.005861508\n",
            "epoch: 805 loss: 0.00585844\n",
            "epoch: 806 loss: 0.005855026\n",
            "epoch: 807 loss: 0.005851301\n",
            "epoch: 808 loss: 0.005847268\n",
            "epoch: 809 loss: 0.005843079\n",
            "epoch: 810 loss: 0.0058388417\n",
            "epoch: 811 loss: 0.0058346987\n",
            "epoch: 812 loss: 0.005830707\n",
            "epoch: 813 loss: 0.00582689\n",
            "epoch: 814 loss: 0.00582324\n",
            "epoch: 815 loss: 0.0058197156\n",
            "epoch: 816 loss: 0.00581626\n",
            "epoch: 817 loss: 0.005812855\n",
            "epoch: 818 loss: 0.005809464\n",
            "epoch: 819 loss: 0.0058060735\n",
            "epoch: 820 loss: 0.0058026877\n",
            "epoch: 821 loss: 0.005799306\n",
            "epoch: 822 loss: 0.0057959524\n",
            "epoch: 823 loss: 0.0057926197\n",
            "epoch: 824 loss: 0.0057893177\n",
            "epoch: 825 loss: 0.005786091\n",
            "epoch: 826 loss: 0.0057829437\n",
            "epoch: 827 loss: 0.005779877\n",
            "epoch: 828 loss: 0.0057769287\n",
            "epoch: 829 loss: 0.0057740933\n",
            "epoch: 830 loss: 0.0057714637\n",
            "epoch: 831 loss: 0.0057688938\n",
            "epoch: 832 loss: 0.0057665273\n",
            "epoch: 833 loss: 0.005764252\n",
            "epoch: 834 loss: 0.005762208\n",
            "epoch: 835 loss: 0.0057599754\n",
            "epoch: 836 loss: 0.005757729\n",
            "epoch: 837 loss: 0.005754871\n",
            "epoch: 838 loss: 0.0057515725\n",
            "epoch: 839 loss: 0.005747168\n",
            "epoch: 840 loss: 0.0057421457\n",
            "epoch: 841 loss: 0.005736308\n",
            "epoch: 842 loss: 0.005730279\n",
            "epoch: 843 loss: 0.0057243225\n",
            "epoch: 844 loss: 0.0057188813\n",
            "epoch: 845 loss: 0.0057141497\n",
            "epoch: 846 loss: 0.005710164\n",
            "epoch: 847 loss: 0.0057068146\n",
            "epoch: 848 loss: 0.0057039172\n",
            "epoch: 849 loss: 0.005701339\n",
            "epoch: 850 loss: 0.005698958\n",
            "epoch: 851 loss: 0.005696721\n",
            "epoch: 852 loss: 0.0056945165\n",
            "epoch: 853 loss: 0.0056923646\n",
            "epoch: 854 loss: 0.005690157\n",
            "epoch: 855 loss: 0.005687906\n",
            "epoch: 856 loss: 0.005685374\n",
            "epoch: 857 loss: 0.0056826603\n",
            "epoch: 858 loss: 0.005679372\n",
            "epoch: 859 loss: 0.005675798\n",
            "epoch: 860 loss: 0.0056716385\n",
            "epoch: 861 loss: 0.005667181\n",
            "epoch: 862 loss: 0.005662285\n",
            "epoch: 863 loss: 0.005657336\n",
            "epoch: 864 loss: 0.005652409\n",
            "epoch: 865 loss: 0.0056477957\n",
            "epoch: 866 loss: 0.005643521\n",
            "epoch: 867 loss: 0.00563963\n",
            "epoch: 868 loss: 0.0056360625\n",
            "epoch: 869 loss: 0.0056327507\n",
            "epoch: 870 loss: 0.005629634\n",
            "epoch: 871 loss: 0.005626671\n",
            "epoch: 872 loss: 0.0056238505\n",
            "epoch: 873 loss: 0.00562117\n",
            "epoch: 874 loss: 0.0056186845\n",
            "epoch: 875 loss: 0.0056164437\n",
            "epoch: 876 loss: 0.0056145303\n",
            "epoch: 877 loss: 0.0056130025\n",
            "epoch: 878 loss: 0.0056120665\n",
            "epoch: 879 loss: 0.0056116437\n",
            "epoch: 880 loss: 0.0056120497\n",
            "epoch: 881 loss: 0.0056126243\n",
            "epoch: 882 loss: 0.0056136395\n",
            "epoch: 883 loss: 0.005613204\n",
            "epoch: 884 loss: 0.005611319\n",
            "epoch: 885 loss: 0.005605754\n",
            "epoch: 886 loss: 0.005597645\n",
            "epoch: 887 loss: 0.0055871718\n",
            "epoch: 888 loss: 0.005577123\n",
            "epoch: 889 loss: 0.0055692773\n",
            "epoch: 890 loss: 0.005564747\n",
            "epoch: 891 loss: 0.005563127\n",
            "epoch: 892 loss: 0.005563149\n",
            "epoch: 893 loss: 0.005563423\n",
            "epoch: 894 loss: 0.0055626477\n",
            "epoch: 895 loss: 0.0055604097\n",
            "epoch: 896 loss: 0.0055561797\n",
            "epoch: 897 loss: 0.0055507068\n",
            "epoch: 898 loss: 0.0055445586\n",
            "epoch: 899 loss: 0.005538752\n",
            "epoch: 900 loss: 0.0055338624\n",
            "epoch: 901 loss: 0.005530178\n",
            "epoch: 902 loss: 0.005527533\n",
            "epoch: 903 loss: 0.005525513\n",
            "epoch: 904 loss: 0.005523711\n",
            "epoch: 905 loss: 0.005521716\n",
            "epoch: 906 loss: 0.005519383\n",
            "epoch: 907 loss: 0.0055165067\n",
            "epoch: 908 loss: 0.005513228\n",
            "epoch: 909 loss: 0.0055095\n",
            "epoch: 910 loss: 0.0055055977\n",
            "epoch: 911 loss: 0.005501605\n",
            "epoch: 912 loss: 0.005497711\n",
            "epoch: 913 loss: 0.005493948\n",
            "epoch: 914 loss: 0.005490398\n",
            "epoch: 915 loss: 0.005487058\n",
            "epoch: 916 loss: 0.005483888\n",
            "epoch: 917 loss: 0.0054808576\n",
            "epoch: 918 loss: 0.0054779383\n",
            "epoch: 919 loss: 0.005475112\n",
            "epoch: 920 loss: 0.0054723774\n",
            "epoch: 921 loss: 0.005469752\n",
            "epoch: 922 loss: 0.0054672468\n",
            "epoch: 923 loss: 0.0054649166\n",
            "epoch: 924 loss: 0.0054628067\n",
            "epoch: 925 loss: 0.0054610246\n",
            "epoch: 926 loss: 0.0054596136\n",
            "epoch: 927 loss: 0.0054588094\n",
            "epoch: 928 loss: 0.0054585556\n",
            "epoch: 929 loss: 0.0054591782\n",
            "epoch: 930 loss: 0.00546027\n",
            "epoch: 931 loss: 0.0054621636\n",
            "epoch: 932 loss: 0.0054631126\n",
            "epoch: 933 loss: 0.005463214\n",
            "epoch: 934 loss: 0.0054596975\n",
            "epoch: 935 loss: 0.005453493\n",
            "epoch: 936 loss: 0.0054431893\n",
            "epoch: 937 loss: 0.0054321126\n",
            "epoch: 938 loss: 0.0054221395\n",
            "epoch: 939 loss: 0.005415516\n",
            "epoch: 940 loss: 0.0054126047\n",
            "epoch: 941 loss: 0.0054124366\n",
            "epoch: 942 loss: 0.0054134442\n",
            "epoch: 943 loss: 0.00541391\n",
            "epoch: 944 loss: 0.0054130903\n",
            "epoch: 945 loss: 0.0054101604\n",
            "epoch: 946 loss: 0.0054055224\n",
            "epoch: 947 loss: 0.0053994353\n",
            "epoch: 948 loss: 0.00539319\n",
            "epoch: 949 loss: 0.0053876024\n",
            "epoch: 950 loss: 0.005383341\n",
            "epoch: 951 loss: 0.0053804494\n",
            "epoch: 952 loss: 0.0053785816\n",
            "epoch: 953 loss: 0.0053772125\n",
            "epoch: 954 loss: 0.005375788\n",
            "epoch: 955 loss: 0.005374022\n",
            "epoch: 956 loss: 0.0053716116\n",
            "epoch: 957 loss: 0.005368686\n",
            "epoch: 958 loss: 0.005365147\n",
            "epoch: 959 loss: 0.0053613153\n",
            "epoch: 960 loss: 0.0053572953\n",
            "epoch: 961 loss: 0.005353402\n",
            "epoch: 962 loss: 0.0053497297\n",
            "epoch: 963 loss: 0.0053463583\n",
            "epoch: 964 loss: 0.0053432756\n",
            "epoch: 965 loss: 0.0053404258\n",
            "epoch: 966 loss: 0.0053377524\n",
            "epoch: 967 loss: 0.00533521\n",
            "epoch: 968 loss: 0.0053327964\n",
            "epoch: 969 loss: 0.0053304923\n",
            "epoch: 970 loss: 0.0053283167\n",
            "epoch: 971 loss: 0.005326281\n",
            "epoch: 972 loss: 0.005324466\n",
            "epoch: 973 loss: 0.0053229127\n",
            "epoch: 974 loss: 0.0053217784\n",
            "epoch: 975 loss: 0.005321013\n",
            "epoch: 976 loss: 0.0053208782\n",
            "epoch: 977 loss: 0.0053210934\n",
            "epoch: 978 loss: 0.005321982\n",
            "epoch: 979 loss: 0.0053225732\n",
            "epoch: 980 loss: 0.005323399\n",
            "epoch: 981 loss: 0.0053222454\n",
            "epoch: 982 loss: 0.0053197965\n",
            "epoch: 983 loss: 0.005313846\n",
            "epoch: 984 loss: 0.0053062555\n",
            "epoch: 985 loss: 0.005296773\n",
            "epoch: 986 loss: 0.005287973\n",
            "epoch: 987 loss: 0.005281057\n",
            "epoch: 988 loss: 0.005276833\n",
            "epoch: 989 loss: 0.005275002\n",
            "epoch: 990 loss: 0.005274676\n",
            "epoch: 991 loss: 0.005274862\n",
            "epoch: 992 loss: 0.005274543\n",
            "epoch: 993 loss: 0.005273313\n",
            "epoch: 994 loss: 0.005270558\n",
            "epoch: 995 loss: 0.0052668303\n",
            "epoch: 996 loss: 0.0052621686\n",
            "epoch: 997 loss: 0.0052573276\n",
            "epoch: 998 loss: 0.005252536\n",
            "epoch: 999 loss: 0.0052482914\n",
            "epoch: 1000 loss: 0.0052447314\n",
            "epoch: 1001 loss: 0.0052418723\n",
            "epoch: 1002 loss: 0.005239558\n",
            "epoch: 1003 loss: 0.0052376105\n",
            "epoch: 1004 loss: 0.005235862\n",
            "epoch: 1005 loss: 0.0052341633\n",
            "epoch: 1006 loss: 0.0052324627\n",
            "epoch: 1007 loss: 0.0052306713\n",
            "epoch: 1008 loss: 0.0052288263\n",
            "epoch: 1009 loss: 0.0052267914\n",
            "epoch: 1010 loss: 0.005224678\n",
            "epoch: 1011 loss: 0.005222294\n",
            "epoch: 1012 loss: 0.0052198456\n",
            "epoch: 1013 loss: 0.0052171443\n",
            "epoch: 1014 loss: 0.0052143694\n",
            "epoch: 1015 loss: 0.005211277\n",
            "epoch: 1016 loss: 0.0052081044\n",
            "epoch: 1017 loss: 0.005204775\n",
            "epoch: 1018 loss: 0.0052014436\n",
            "epoch: 1019 loss: 0.005198088\n",
            "epoch: 1020 loss: 0.0051948265\n",
            "epoch: 1021 loss: 0.005191623\n",
            "epoch: 1022 loss: 0.005188563\n",
            "epoch: 1023 loss: 0.0051856046\n",
            "epoch: 1024 loss: 0.0051827678\n",
            "epoch: 1025 loss: 0.0051800073\n",
            "epoch: 1026 loss: 0.005177326\n",
            "epoch: 1027 loss: 0.005174708\n",
            "epoch: 1028 loss: 0.005172172\n",
            "epoch: 1029 loss: 0.005169705\n",
            "epoch: 1030 loss: 0.0051673683\n",
            "epoch: 1031 loss: 0.0051651946\n",
            "epoch: 1032 loss: 0.005163288\n",
            "epoch: 1033 loss: 0.0051617366\n",
            "epoch: 1034 loss: 0.005160837\n",
            "epoch: 1035 loss: 0.005160861\n",
            "epoch: 1036 loss: 0.005162529\n",
            "epoch: 1037 loss: 0.005166186\n",
            "epoch: 1038 loss: 0.0051733116\n",
            "epoch: 1039 loss: 0.005182916\n",
            "epoch: 1040 loss: 0.0051971753\n",
            "epoch: 1041 loss: 0.0052076695\n",
            "epoch: 1042 loss: 0.0052150134\n",
            "epoch: 1043 loss: 0.005202182\n",
            "epoch: 1044 loss: 0.005177622\n",
            "epoch: 1045 loss: 0.0051457733\n",
            "epoch: 1046 loss: 0.0051270067\n",
            "epoch: 1047 loss: 0.0051276283\n",
            "epoch: 1048 loss: 0.0051401616\n",
            "epoch: 1049 loss: 0.005151334\n",
            "epoch: 1050 loss: 0.005148832\n",
            "epoch: 1051 loss: 0.005135295\n",
            "epoch: 1052 loss: 0.005117998\n",
            "epoch: 1053 loss: 0.0051087607\n",
            "epoch: 1054 loss: 0.005110202\n",
            "epoch: 1055 loss: 0.0051162424\n",
            "epoch: 1056 loss: 0.0051189456\n",
            "epoch: 1057 loss: 0.0051135044\n",
            "epoch: 1058 loss: 0.0051037506\n",
            "epoch: 1059 loss: 0.0050951694\n",
            "epoch: 1060 loss: 0.005092001\n",
            "epoch: 1061 loss: 0.005093266\n",
            "epoch: 1062 loss: 0.0050947624\n",
            "epoch: 1063 loss: 0.0050933263\n",
            "epoch: 1064 loss: 0.0050881687\n",
            "epoch: 1065 loss: 0.0050819716\n",
            "epoch: 1066 loss: 0.0050772396\n",
            "epoch: 1067 loss: 0.0050751506\n",
            "epoch: 1068 loss: 0.00507477\n",
            "epoch: 1069 loss: 0.0050742174\n",
            "epoch: 1070 loss: 0.0050722337\n",
            "epoch: 1071 loss: 0.0050685727\n",
            "epoch: 1072 loss: 0.005064305\n",
            "epoch: 1073 loss: 0.0050605224\n",
            "epoch: 1074 loss: 0.005057848\n",
            "epoch: 1075 loss: 0.005056112\n",
            "epoch: 1076 loss: 0.005054694\n",
            "epoch: 1077 loss: 0.0050529703\n",
            "epoch: 1078 loss: 0.0050505563\n",
            "epoch: 1079 loss: 0.005047604\n",
            "epoch: 1080 loss: 0.005044402\n",
            "epoch: 1081 loss: 0.0050413595\n",
            "epoch: 1082 loss: 0.005038697\n",
            "epoch: 1083 loss: 0.00503643\n",
            "epoch: 1084 loss: 0.005034408\n",
            "epoch: 1085 loss: 0.005032431\n",
            "epoch: 1086 loss: 0.0050303494\n",
            "epoch: 1087 loss: 0.0050280546\n",
            "epoch: 1088 loss: 0.0050255847\n",
            "epoch: 1089 loss: 0.005022962\n",
            "epoch: 1090 loss: 0.005020298\n",
            "epoch: 1091 loss: 0.0050176615\n",
            "epoch: 1092 loss: 0.0050151064\n",
            "epoch: 1093 loss: 0.005012656\n",
            "epoch: 1094 loss: 0.0050102966\n",
            "epoch: 1095 loss: 0.005008004\n",
            "epoch: 1096 loss: 0.0050057573\n",
            "epoch: 1097 loss: 0.0050035343\n",
            "epoch: 1098 loss: 0.0050013266\n",
            "epoch: 1099 loss: 0.00499912\n",
            "epoch: 1100 loss: 0.004996915\n",
            "epoch: 1101 loss: 0.004994705\n",
            "epoch: 1102 loss: 0.0049924958\n",
            "epoch: 1103 loss: 0.0049903016\n",
            "epoch: 1104 loss: 0.004988129\n",
            "epoch: 1105 loss: 0.0049860063\n",
            "epoch: 1106 loss: 0.0049839104\n",
            "epoch: 1107 loss: 0.0049818885\n",
            "epoch: 1108 loss: 0.0049799257\n",
            "epoch: 1109 loss: 0.004978113\n",
            "epoch: 1110 loss: 0.004976413\n",
            "epoch: 1111 loss: 0.004974967\n",
            "epoch: 1112 loss: 0.0049737333\n",
            "epoch: 1113 loss: 0.004972966\n",
            "epoch: 1114 loss: 0.004972514\n",
            "epoch: 1115 loss: 0.0049728723\n",
            "epoch: 1116 loss: 0.0049735494\n",
            "epoch: 1117 loss: 0.0049754432\n",
            "epoch: 1118 loss: 0.0049771513\n",
            "epoch: 1119 loss: 0.004979962\n",
            "epoch: 1120 loss: 0.0049808742\n",
            "epoch: 1121 loss: 0.004981547\n",
            "epoch: 1122 loss: 0.004977566\n",
            "epoch: 1123 loss: 0.0049717994\n",
            "epoch: 1124 loss: 0.0049615973\n",
            "epoch: 1125 loss: 0.0049511124\n",
            "epoch: 1126 loss: 0.0049414854\n",
            "epoch: 1127 loss: 0.004935004\n",
            "epoch: 1128 loss: 0.004932007\n",
            "epoch: 1129 loss: 0.004931794\n",
            "epoch: 1130 loss: 0.00493311\n",
            "epoch: 1131 loss: 0.004934437\n",
            "epoch: 1132 loss: 0.0049351067\n",
            "epoch: 1133 loss: 0.0049338075\n",
            "epoch: 1134 loss: 0.004931144\n",
            "epoch: 1135 loss: 0.004926421\n",
            "epoch: 1136 loss: 0.004921075\n",
            "epoch: 1137 loss: 0.004915535\n",
            "epoch: 1138 loss: 0.0049108486\n",
            "epoch: 1139 loss: 0.0049073636\n",
            "epoch: 1140 loss: 0.0049050953\n",
            "epoch: 1141 loss: 0.004903734\n",
            "epoch: 1142 loss: 0.004902843\n",
            "epoch: 1143 loss: 0.0049021016\n",
            "epoch: 1144 loss: 0.0049011116\n",
            "epoch: 1145 loss: 0.004899889\n",
            "epoch: 1146 loss: 0.004898108\n",
            "epoch: 1147 loss: 0.004896048\n",
            "epoch: 1148 loss: 0.004893391\n",
            "epoch: 1149 loss: 0.004890579\n",
            "epoch: 1150 loss: 0.0048874775\n",
            "epoch: 1151 loss: 0.004884386\n",
            "epoch: 1152 loss: 0.0048812483\n",
            "epoch: 1153 loss: 0.0048782616\n",
            "epoch: 1154 loss: 0.0048753857\n",
            "epoch: 1155 loss: 0.004872682\n",
            "epoch: 1156 loss: 0.0048701223\n",
            "epoch: 1157 loss: 0.004867687\n",
            "epoch: 1158 loss: 0.004865335\n",
            "epoch: 1159 loss: 0.004863051\n",
            "epoch: 1160 loss: 0.0048608095\n",
            "epoch: 1161 loss: 0.0048585935\n",
            "epoch: 1162 loss: 0.004856395\n",
            "epoch: 1163 loss: 0.00485421\n",
            "epoch: 1164 loss: 0.004852031\n",
            "epoch: 1165 loss: 0.0048498563\n",
            "epoch: 1166 loss: 0.0048476877\n",
            "epoch: 1167 loss: 0.004845534\n",
            "epoch: 1168 loss: 0.0048434082\n",
            "epoch: 1169 loss: 0.0048413165\n",
            "epoch: 1170 loss: 0.0048392946\n",
            "epoch: 1171 loss: 0.0048373886\n",
            "epoch: 1172 loss: 0.004835713\n",
            "epoch: 1173 loss: 0.004834446\n",
            "epoch: 1174 loss: 0.0048339325\n",
            "epoch: 1175 loss: 0.0048346664\n",
            "epoch: 1176 loss: 0.004837833\n",
            "epoch: 1177 loss: 0.0048444597\n",
            "epoch: 1178 loss: 0.004858175\n",
            "epoch: 1179 loss: 0.004878404\n",
            "epoch: 1180 loss: 0.0049128975\n",
            "epoch: 1181 loss: 0.0049409755\n",
            "epoch: 1182 loss: 0.0049672686\n",
            "epoch: 1183 loss: 0.0049355677\n",
            "epoch: 1184 loss: 0.0048787114\n",
            "epoch: 1185 loss: 0.0048195454\n",
            "epoch: 1186 loss: 0.0048100534\n",
            "epoch: 1187 loss: 0.0048424215\n",
            "epoch: 1188 loss: 0.0048675495\n",
            "epoch: 1189 loss: 0.0048600766\n",
            "epoch: 1190 loss: 0.004820261\n",
            "epoch: 1191 loss: 0.0047958368\n",
            "epoch: 1192 loss: 0.0048044235\n",
            "epoch: 1193 loss: 0.0048240474\n",
            "epoch: 1194 loss: 0.004826403\n",
            "epoch: 1195 loss: 0.004804607\n",
            "epoch: 1196 loss: 0.004786273\n",
            "epoch: 1197 loss: 0.0047876933\n",
            "epoch: 1198 loss: 0.0047990554\n",
            "epoch: 1199 loss: 0.004801454\n",
            "epoch: 1200 loss: 0.0047886395\n",
            "epoch: 1201 loss: 0.0047760797\n",
            "epoch: 1202 loss: 0.004774936\n",
            "epoch: 1203 loss: 0.004780877\n",
            "epoch: 1204 loss: 0.0047821733\n",
            "epoch: 1205 loss: 0.0047743893\n",
            "epoch: 1206 loss: 0.0047657783\n",
            "epoch: 1207 loss: 0.0047634505\n",
            "epoch: 1208 loss: 0.0047659865\n",
            "epoch: 1209 loss: 0.004766789\n",
            "epoch: 1210 loss: 0.004762228\n",
            "epoch: 1211 loss: 0.0047560004\n",
            "epoch: 1212 loss: 0.004752587\n",
            "epoch: 1213 loss: 0.004752693\n",
            "epoch: 1214 loss: 0.004753145\n",
            "epoch: 1215 loss: 0.004750913\n",
            "epoch: 1216 loss: 0.004746621\n",
            "epoch: 1217 loss: 0.0047426834\n",
            "epoch: 1218 loss: 0.00474082\n",
            "epoch: 1219 loss: 0.004740331\n",
            "epoch: 1220 loss: 0.004739315\n",
            "epoch: 1221 loss: 0.0047368715\n",
            "epoch: 1222 loss: 0.004733505\n",
            "epoch: 1223 loss: 0.004730565\n",
            "epoch: 1224 loss: 0.0047287145\n",
            "epoch: 1225 loss: 0.004727538\n",
            "epoch: 1226 loss: 0.004726144\n",
            "epoch: 1227 loss: 0.0047239824\n",
            "epoch: 1228 loss: 0.0047213067\n",
            "epoch: 1229 loss: 0.004718701\n",
            "epoch: 1230 loss: 0.0047165975\n",
            "epoch: 1231 loss: 0.0047149514\n",
            "epoch: 1232 loss: 0.0047133733\n",
            "epoch: 1233 loss: 0.0047115446\n",
            "epoch: 1234 loss: 0.004709369\n",
            "epoch: 1235 loss: 0.0047070268\n",
            "epoch: 1236 loss: 0.0047047683\n",
            "epoch: 1237 loss: 0.004702749\n",
            "epoch: 1238 loss: 0.004700928\n",
            "epoch: 1239 loss: 0.00469916\n",
            "epoch: 1240 loss: 0.004697299\n",
            "epoch: 1241 loss: 0.0046952767\n",
            "epoch: 1242 loss: 0.0046931584\n",
            "epoch: 1243 loss: 0.0046910313\n",
            "epoch: 1244 loss: 0.0046889805\n",
            "epoch: 1245 loss: 0.0046870247\n",
            "epoch: 1246 loss: 0.0046851374\n",
            "epoch: 1247 loss: 0.004683268\n",
            "epoch: 1248 loss: 0.004681368\n",
            "epoch: 1249 loss: 0.0046794233\n",
            "epoch: 1250 loss: 0.0046774307\n",
            "epoch: 1251 loss: 0.004675415\n",
            "epoch: 1252 loss: 0.004673401\n",
            "epoch: 1253 loss: 0.0046714055\n",
            "epoch: 1254 loss: 0.004669436\n",
            "epoch: 1255 loss: 0.0046674926\n",
            "epoch: 1256 loss: 0.0046655685\n",
            "epoch: 1257 loss: 0.0046636523\n",
            "epoch: 1258 loss: 0.0046617407\n",
            "epoch: 1259 loss: 0.0046598227\n",
            "epoch: 1260 loss: 0.004657896\n",
            "epoch: 1261 loss: 0.0046559563\n",
            "epoch: 1262 loss: 0.0046540224\n",
            "epoch: 1263 loss: 0.004652074\n",
            "epoch: 1264 loss: 0.0046501267\n",
            "epoch: 1265 loss: 0.004648173\n",
            "epoch: 1266 loss: 0.0046462384\n",
            "epoch: 1267 loss: 0.0046442887\n",
            "epoch: 1268 loss: 0.004642351\n",
            "epoch: 1269 loss: 0.0046404167\n",
            "epoch: 1270 loss: 0.004638497\n",
            "epoch: 1271 loss: 0.004636564\n",
            "epoch: 1272 loss: 0.004634647\n",
            "epoch: 1273 loss: 0.0046327203\n",
            "epoch: 1274 loss: 0.004630807\n",
            "epoch: 1275 loss: 0.0046288893\n",
            "epoch: 1276 loss: 0.0046269833\n",
            "epoch: 1277 loss: 0.0046250797\n",
            "epoch: 1278 loss: 0.0046231914\n",
            "epoch: 1279 loss: 0.0046213134\n",
            "epoch: 1280 loss: 0.0046194578\n",
            "epoch: 1281 loss: 0.0046176333\n",
            "epoch: 1282 loss: 0.004615871\n",
            "epoch: 1283 loss: 0.004614163\n",
            "epoch: 1284 loss: 0.0046125846\n",
            "epoch: 1285 loss: 0.0046111606\n",
            "epoch: 1286 loss: 0.0046100453\n",
            "epoch: 1287 loss: 0.004609265\n",
            "epoch: 1288 loss: 0.0046091774\n",
            "epoch: 1289 loss: 0.0046098097\n",
            "epoch: 1290 loss: 0.004612049\n",
            "epoch: 1291 loss: 0.0046155835\n",
            "epoch: 1292 loss: 0.004622428\n",
            "epoch: 1293 loss: 0.004630567\n",
            "epoch: 1294 loss: 0.0046438593\n",
            "epoch: 1295 loss: 0.004653284\n",
            "epoch: 1296 loss: 0.004664956\n",
            "epoch: 1297 loss: 0.0046590525\n",
            "epoch: 1298 loss: 0.0046471166\n",
            "epoch: 1299 loss: 0.0046193167\n",
            "epoch: 1300 loss: 0.004594832\n",
            "epoch: 1301 loss: 0.0045809536\n",
            "epoch: 1302 loss: 0.0045818104\n",
            "epoch: 1303 loss: 0.004592019\n",
            "epoch: 1304 loss: 0.004601454\n",
            "epoch: 1305 loss: 0.004605045\n",
            "epoch: 1306 loss: 0.0045966874\n",
            "epoch: 1307 loss: 0.004583865\n",
            "epoch: 1308 loss: 0.0045713186\n",
            "epoch: 1309 loss: 0.004565681\n",
            "epoch: 1310 loss: 0.0045672064\n",
            "epoch: 1311 loss: 0.0045717657\n",
            "epoch: 1312 loss: 0.0045748954\n",
            "epoch: 1313 loss: 0.0045725233\n",
            "epoch: 1314 loss: 0.0045665293\n",
            "epoch: 1315 loss: 0.004558898\n",
            "epoch: 1316 loss: 0.0045534573\n",
            "epoch: 1317 loss: 0.0045515294\n",
            "epoch: 1318 loss: 0.004552228\n",
            "epoch: 1319 loss: 0.004553522\n",
            "epoch: 1320 loss: 0.004553218\n",
            "epoch: 1321 loss: 0.004550998\n",
            "epoch: 1322 loss: 0.0045469566\n",
            "epoch: 1323 loss: 0.0045426735\n",
            "epoch: 1324 loss: 0.004539051\n",
            "epoch: 1325 loss: 0.0045367456\n",
            "epoch: 1326 loss: 0.004535634\n",
            "epoch: 1327 loss: 0.004535075\n",
            "epoch: 1328 loss: 0.004534424\n",
            "epoch: 1329 loss: 0.004533078\n",
            "epoch: 1330 loss: 0.0045310915\n",
            "epoch: 1331 loss: 0.004528495\n",
            "epoch: 1332 loss: 0.0045257513\n",
            "epoch: 1333 loss: 0.004523104\n",
            "epoch: 1334 loss: 0.0045208056\n",
            "epoch: 1335 loss: 0.004518918\n",
            "epoch: 1336 loss: 0.0045173676\n",
            "epoch: 1337 loss: 0.0045160125\n",
            "epoch: 1338 loss: 0.004514703\n",
            "epoch: 1339 loss: 0.004513341\n",
            "epoch: 1340 loss: 0.0045118625\n",
            "epoch: 1341 loss: 0.004510255\n",
            "epoch: 1342 loss: 0.00450849\n",
            "epoch: 1343 loss: 0.0045066173\n",
            "epoch: 1344 loss: 0.004504627\n",
            "epoch: 1345 loss: 0.0045026205\n",
            "epoch: 1346 loss: 0.0045006\n",
            "epoch: 1347 loss: 0.0044986014\n",
            "epoch: 1348 loss: 0.0044966117\n",
            "epoch: 1349 loss: 0.004494685\n",
            "epoch: 1350 loss: 0.0044927956\n",
            "epoch: 1351 loss: 0.0044909576\n",
            "epoch: 1352 loss: 0.004489146\n",
            "epoch: 1353 loss: 0.004487362\n",
            "epoch: 1354 loss: 0.0044855974\n",
            "epoch: 1355 loss: 0.004483841\n",
            "epoch: 1356 loss: 0.0044820956\n",
            "epoch: 1357 loss: 0.004480356\n",
            "epoch: 1358 loss: 0.004478622\n",
            "epoch: 1359 loss: 0.0044768876\n",
            "epoch: 1360 loss: 0.0044751577\n",
            "epoch: 1361 loss: 0.004473427\n",
            "epoch: 1362 loss: 0.0044716992\n",
            "epoch: 1363 loss: 0.0044699726\n",
            "epoch: 1364 loss: 0.004468251\n",
            "epoch: 1365 loss: 0.0044665285\n",
            "epoch: 1366 loss: 0.0044648093\n",
            "epoch: 1367 loss: 0.004463092\n",
            "epoch: 1368 loss: 0.004461377\n",
            "epoch: 1369 loss: 0.0044596656\n",
            "epoch: 1370 loss: 0.004457951\n",
            "epoch: 1371 loss: 0.0044562467\n",
            "epoch: 1372 loss: 0.0044545424\n",
            "epoch: 1373 loss: 0.0044528474\n",
            "epoch: 1374 loss: 0.0044511636\n",
            "epoch: 1375 loss: 0.0044494984\n",
            "epoch: 1376 loss: 0.004447894\n",
            "epoch: 1377 loss: 0.0044463663\n",
            "epoch: 1378 loss: 0.004445001\n",
            "epoch: 1379 loss: 0.004444029\n",
            "epoch: 1380 loss: 0.0044436553\n",
            "epoch: 1381 loss: 0.0044447845\n",
            "epoch: 1382 loss: 0.0044484613\n",
            "epoch: 1383 loss: 0.004457855\n",
            "epoch: 1384 loss: 0.004475215\n",
            "epoch: 1385 loss: 0.004512112\n",
            "epoch: 1386 loss: 0.0045616813\n",
            "epoch: 1387 loss: 0.004647652\n",
            "epoch: 1388 loss: 0.004669643\n",
            "epoch: 1389 loss: 0.004653924\n",
            "epoch: 1390 loss: 0.00451409\n",
            "epoch: 1391 loss: 0.0044286237\n",
            "epoch: 1392 loss: 0.004459524\n",
            "epoch: 1393 loss: 0.0045287735\n",
            "epoch: 1394 loss: 0.0045382814\n",
            "epoch: 1395 loss: 0.004457231\n",
            "epoch: 1396 loss: 0.0044171386\n",
            "epoch: 1397 loss: 0.0044538244\n",
            "epoch: 1398 loss: 0.004484586\n",
            "epoch: 1399 loss: 0.004460042\n",
            "epoch: 1400 loss: 0.0044146553\n",
            "epoch: 1401 loss: 0.0044208956\n",
            "epoch: 1402 loss: 0.0044520157\n",
            "epoch: 1403 loss: 0.0044414722\n",
            "epoch: 1404 loss: 0.0044103744\n",
            "epoch: 1405 loss: 0.004405989\n",
            "epoch: 1406 loss: 0.004424738\n",
            "epoch: 1407 loss: 0.0044266335\n",
            "epoch: 1408 loss: 0.00440484\n",
            "epoch: 1409 loss: 0.0043967534\n",
            "epoch: 1410 loss: 0.0044081607\n",
            "epoch: 1411 loss: 0.0044108867\n",
            "epoch: 1412 loss: 0.004398335\n",
            "epoch: 1413 loss: 0.0043894337\n",
            "epoch: 1414 loss: 0.004394502\n",
            "epoch: 1415 loss: 0.0043991683\n",
            "epoch: 1416 loss: 0.0043918565\n",
            "epoch: 1417 loss: 0.0043833996\n",
            "epoch: 1418 loss: 0.004383922\n",
            "epoch: 1419 loss: 0.0043875403\n",
            "epoch: 1420 loss: 0.004385006\n",
            "epoch: 1421 loss: 0.0043781647\n",
            "epoch: 1422 loss: 0.0043756226\n",
            "epoch: 1423 loss: 0.0043775444\n",
            "epoch: 1424 loss: 0.0043771747\n",
            "epoch: 1425 loss: 0.004372829\n",
            "epoch: 1426 loss: 0.0043689893\n",
            "epoch: 1427 loss: 0.004368697\n",
            "epoch: 1428 loss: 0.004369179\n",
            "epoch: 1429 loss: 0.0043670177\n",
            "epoch: 1430 loss: 0.004363428\n",
            "epoch: 1431 loss: 0.0043613287\n",
            "epoch: 1432 loss: 0.0043610404\n",
            "epoch: 1433 loss: 0.004360334\n",
            "epoch: 1434 loss: 0.0043579526\n",
            "epoch: 1435 loss: 0.0043552704\n",
            "epoch: 1436 loss: 0.004353768\n",
            "epoch: 1437 loss: 0.004353089\n",
            "epoch: 1438 loss: 0.004351877\n",
            "epoch: 1439 loss: 0.004349723\n",
            "epoch: 1440 loss: 0.0043475484\n",
            "epoch: 1441 loss: 0.0043460974\n",
            "epoch: 1442 loss: 0.004345087\n",
            "epoch: 1443 loss: 0.004343752\n",
            "epoch: 1444 loss: 0.0043418817\n",
            "epoch: 1445 loss: 0.0043399674\n",
            "epoch: 1446 loss: 0.004338439\n",
            "epoch: 1447 loss: 0.004337212\n",
            "epoch: 1448 loss: 0.004335877\n",
            "epoch: 1449 loss: 0.0043342267\n",
            "epoch: 1450 loss: 0.0043324633\n",
            "epoch: 1451 loss: 0.004330862\n",
            "epoch: 1452 loss: 0.004329478\n",
            "epoch: 1453 loss: 0.0043281303\n",
            "epoch: 1454 loss: 0.0043266322\n",
            "epoch: 1455 loss: 0.0043249954\n",
            "epoch: 1456 loss: 0.004323376\n",
            "epoch: 1457 loss: 0.0043218858\n",
            "epoch: 1458 loss: 0.0043204892\n",
            "epoch: 1459 loss: 0.004319061\n",
            "epoch: 1460 loss: 0.0043175416\n",
            "epoch: 1461 loss: 0.004315968\n",
            "epoch: 1462 loss: 0.0043144193\n",
            "epoch: 1463 loss: 0.004312935\n",
            "epoch: 1464 loss: 0.0043115\n",
            "epoch: 1465 loss: 0.0043100542\n",
            "epoch: 1466 loss: 0.004308565\n",
            "epoch: 1467 loss: 0.004307044\n",
            "epoch: 1468 loss: 0.0043055248\n",
            "epoch: 1469 loss: 0.0043040323\n",
            "epoch: 1470 loss: 0.004302575\n",
            "epoch: 1471 loss: 0.0043011247\n",
            "epoch: 1472 loss: 0.0042996635\n",
            "epoch: 1473 loss: 0.0042981766\n",
            "epoch: 1474 loss: 0.0042966804\n",
            "epoch: 1475 loss: 0.0042951885\n",
            "epoch: 1476 loss: 0.0042937137\n",
            "epoch: 1477 loss: 0.0042922567\n",
            "epoch: 1478 loss: 0.0042908057\n",
            "epoch: 1479 loss: 0.004289353\n",
            "epoch: 1480 loss: 0.004287885\n",
            "epoch: 1481 loss: 0.004286407\n",
            "epoch: 1482 loss: 0.0042849276\n",
            "epoch: 1483 loss: 0.0042834575\n",
            "epoch: 1484 loss: 0.0042819963\n",
            "epoch: 1485 loss: 0.0042805416\n",
            "epoch: 1486 loss: 0.0042790906\n",
            "epoch: 1487 loss: 0.004277643\n",
            "epoch: 1488 loss: 0.0042761834\n",
            "epoch: 1489 loss: 0.0042747282\n",
            "epoch: 1490 loss: 0.004273262\n",
            "epoch: 1491 loss: 0.0042718085\n",
            "epoch: 1492 loss: 0.004270351\n",
            "epoch: 1493 loss: 0.00426891\n",
            "epoch: 1494 loss: 0.004267458\n",
            "epoch: 1495 loss: 0.0042660167\n",
            "epoch: 1496 loss: 0.0042645657\n",
            "epoch: 1497 loss: 0.004263123\n",
            "epoch: 1498 loss: 0.004261669\n",
            "epoch: 1499 loss: 0.004260225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZgPycJ_zhRO",
        "colab_type": "code",
        "outputId": "2b8a91fe-6c5b-422c-92b2-d7e89879b069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import skimage.color as color\n",
        "import skimage.io as io\n",
        "import os\n",
        "test_path = '/content/test_img'\n",
        "img = img_to_array(load_img(test_path+'/'+str(1)+'.png'))\n",
        "img = resize(img,(256,256))\n",
        "print(img.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 256, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTdZR0Lw6P8W",
        "colab_type": "code",
        "outputId": "738acc8c-3046-49c2-a57d-5f2d7f30f5c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "output = session.run(conv13, feed_dict = {x: X[4].reshape([1, 256, 256, 1])})*128\n",
        "image = np.zeros([256, 256, 3])\n",
        "image[:,:,0]=X[0][:,:,0]\n",
        "image[:,:,1:]=output[0]\n",
        "image = color.lab2rgb(image)\n",
        "io.imsave(\"test_img6.jpg\", image)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZkdhCcQ0n9t",
        "colab_type": "code",
        "outputId": "407e7b1c-602f-459a-87c1-a294c21786ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "\n",
        "test_path = '/content/test_img'\n",
        "for i in range(1,6):\n",
        "  color_me = []\n",
        "  img = img_to_array(load_img(test_path+'/'+str(i)+'.png'))\n",
        "  img = resize(img,(256,256))\n",
        "  color_me.append(img)\n",
        "  color_me = np.array(color_me, dtype=float)\n",
        "  color_me = rgb2lab(1.0/255*color_me)[:,:,:,0]\n",
        "  color_me = color_me.reshape(color_me.shape+(1,))\n",
        "  output = session.run(conv13, feed_dict = {x: X[0].reshape([1, 256, 256, 1])})*128\n",
        "  image = np.zeros([256, 256, 3])\n",
        "  image[:,:,0]=X[0][:,:,0]\n",
        "  image[:,:,1:]=output[0]\n",
        "  '''output = output * 128\n",
        "  result = np.zeros((256, 256, 3))\n",
        "  result[:,:,0] = color_me[0][:,:,0]\n",
        "  result[:,:,1:] = output[0]'''\n",
        "  imsave('img_model2_'+str(i+1)+'.png',lab2rgb(result))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}